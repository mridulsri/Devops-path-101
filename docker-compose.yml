# version: '3.8'
name: devops_path_101

networks:
  backend:
    driver: bridge
  frontend:
    driver: bridge

services:
  # ================= PostgreSQL =================
  postgres:
    image: postgres:17
    container_name: postgres_container
    profiles: ["sqldb"] # Optional: to run with specific profile
    restart: unless-stopped
    env_file:
      - ./local-data/.env
    environment:
      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD}
      POSTGRES_USER: ${POSTGRES_USER}      
      POSTGRES_DB: ${POSTGRES_DB}
    volumes:
      - ./local-data/postgres/data:/var/lib/postgresql/data
      # - ./postgres/init:/docker-entrypoint-initdb.d  # optional init scripts
    ports:
      - "5431:5432"  # Remove in prod if not needed externally
    networks:
      - backend
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U ${POSTGRES_USER} -d ${POSTGRES_DB}"]
      interval: 30s
      timeout: 10s
      retries: 3
    deploy:
      resources:
        limits:
          memory: 1G
          cpus: '0.5'
  # ================= pgAdmin =================
  pgadmin:
    image: dpage/pgadmin4
    container_name: pgadmin_container
    profiles: ["sqldb"]
    restart: unless-stopped
    env_file:
      - ./local-data/.env
    environment:
      PGADMIN_DEFAULT_EMAIL: ${PGADMIN_DEFAULT_EMAIL}
      PGADMIN_DEFAULT_PASSWORD: ${PGADMIN_DEFAULT_PASSWORD}
      PGADMIN_LISTEN_PORT: 80
    ports:
      - "8181:80" # Map host port 8080 to container port 80
    volumes:
      - ./local-data/pgadmin:/var/lib/pgadmin
    networks:
      - backend
    depends_on:
      - postgres

  # ================= MongoDB =================
  mongodb:
    image: mongo:8.2.3-noble
    container_name: mongodb_container
    restart: unless-stopped
    env_file:
      - ./local-data/.env
    environment:
      MONGO_INITDB_ROOT_USERNAME: ${MONGO_ROOT_USERNAME}
      MONGO_INITDB_ROOT_PASSWORD: ${MONGO_ROOT_PASSWORD}
    volumes:
      - ./local-data/mongodb/data/db:/data/db
      - ./local-data/mongodb/data/configdb:/data/configdb
    ports:
      - "27018:27017"  # Remove in prod if not needed externally
    networks:
      - backend
    healthcheck:
      test: echo 'db.runCommand("ping").ok' | mongosh localhost:27017/test --quiet
      interval: 30s
      timeout: 10s
      retries: 3
    deploy:
      resources:
        limits:
          memory: 1G
          cpus: '0.5'

  # ================= Elasticsearch =================
  elasticsearch:
    image: docker.elastic.co/elasticsearch/elasticsearch:8.11.0
    container_name: elasticsearch_container
    restart: unless-stopped
    env_file:
      - ./local-data/.env
    environment:
      - discovery.type=single-node
      - xpack.security.enabled=true
      - ELASTIC_PASSWORD=${ELASTIC_PASSWORD}
      - bootstrap.memory_lock=true
      - "ES_JAVA_OPTS=-Xms1g -Xmx1g"
    ulimits:
      memlock:
        soft: -1
        hard: -1
    volumes:
      - ./local-data/elastic/data:/usr/share/elasticsearch/data
    ports:
      - "9200:9200"
    networks:
      - backend
    healthcheck:
      test: ["CMD-SHELL", "curl -s -u elastic:${ELASTIC_PASSWORD} http://localhost:9200/_cluster/health | grep -q '\"status\":\"green\"'"]
      interval: 30s
      timeout: 10s
      retries: 5
    deploy:
      resources:
        limits:
          memory: 2G
          cpus: '1'

  # ================= Logstash =================
  logstash:
    image: docker.elastic.co/logstash/logstash:8.11.0
    container_name: logstash_container
    restart: unless-stopped
    env_file:
      - ./local-data/.env
    volumes:
      - ./logstash_101/pipeline:/usr/share/logstash/pipeline
      - ./logstash_101/config/logstash.yml:/usr/share/logstash/config/logstash.yml
    ports:
      - "5044:5044"  # Beats input
      - "9600:9600"  # Monitoring
    environment:
      - xpack.monitoring.enabled=false
    networks:
      - backend
    depends_on:
      elasticsearch:
        condition: service_healthy
    deploy:
      resources:
        limits:
          memory: 1G
          cpus: '0.5'

  # ================= Kibana =================
  kibana:
    image: docker.elastic.co/kibana/kibana:8.11.0
    container_name: kibana_container
    restart: unless-stopped
    env_file:
      - ./local-data/.env
    environment:
      - SERVER_HOST=0.0.0.0
      - SERVER_PORT=5601
      - ELASTICSEARCH_HOSTS=http://elasticsearch:9200
      - ELASTICSEARCH_USERNAME=elastic
      - ELASTICSEARCH_PASSWORD=${ELASTIC_PASSWORD}
      - KIBANA_PASSWORD=${KIBANA_PASSWORD}
    ports:
      - "5601:5601"
    networks:
      - backend
      - frontend
    depends_on:
      elasticsearch:
        condition: service_healthy
    healthcheck:
      test: curl -s -I http://localhost:5601 | grep -q "HTTP/1.1 302"
      interval: 30s
      timeout: 10s
      retries: 5
    deploy:
      resources:
        limits:
          memory: 1G
          cpus: '0.5'

  # ================= Redis =================
  redis:
    image: redis:7-alpine
    container_name: redis_container
    restart: unless-stopped
    env_file:
      - ./local-data/.env
    command: redis-server --requirepass ${REDIS_PASSWORD} --appendonly yes
    volumes:
      - ./local-data/redis/data:/data
    ports:
      - "6378:6379"  # Remove in prod if not needed externally
    networks:
      - backend
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 30s
      timeout: 10s
      retries: 3
    deploy:
      resources:
        limits:
          memory: 512M
          cpus: '0.25'

  # ================= RabbitMQ =================
  rabbitmq:
    image: rabbitmq:3.12-management-alpine
    container_name: rabbitmq_container
    restart: unless-stopped
    env_file:
      - ./local-data/.env
    environment:
      RABBITMQ_DEFAULT_USER: ${RABBITMQ_DEFAULT_USER}
      RABBITMQ_DEFAULT_PASS: ${RABBITMQ_DEFAULT_PASS}
      RABBITMQ_VM_MEMORY_HIGH_WATERMARK: 0.4
    volumes:
      - ./local-data/rabbitmq/data:/var/lib/rabbitmq
      - ./rabbitmq_101/rabbitmq.conf:/etc/rabbitmq/rabbitmq.conf
    ports:
      - "5672:5672"
      - "15672:15672"  # Management UI
    networks:
      - backend
    healthcheck:
      test: rabbitmq-diagnostics -q ping
      interval: 30s
      timeout: 30s
      retries: 3
    deploy:
      resources:
        limits:
          memory: 1G
          cpus: '0.5'

  # ================= Nginx (Reverse Proxy + SSL) =================
  nginx:
    image: nginx:1.25-alpine
    container_name: nginx_container
    restart: unless-stopped
    env_file:
      - ./local-data/.env
    ports:
      - "80:80"
      - "443:443"
    volumes:
      - ./nginx_101/nginx_html_proj/nginx:/usr/share/nginx/html
      - ./nginx_101/nginx_certs:/etc/letsencrypt
      - ./nginx_101/nginx_config/nginx.conf:/etc/nginx/nginx.conf:ro
      - ./nginx_101/nginx_config/conf.d:/etc/nginx/conf.d:ro
      - ./nginx_101/nginx_config/nginx.conf:/etc/nginx/user.conf.d:ro
    networks:
      - frontend
      - backend
    depends_on:
      - kibana
      # - postgres
      - mongodb
      - elasticsearch
      - redis
      - rabbitmq
    healthcheck:
      test: ["CMD", "nginx", "-t"]
      interval: 1m
      timeout: 10s
      retries: 3
    deploy:
      resources:
        limits:
          memory: 256M
          cpus: '0.25'

  # ================= Certbot (for Let's Encrypt SSL) =================
  certbot:
    image: certbot/certbot
    container_name: certbot_container
    restart: unless-stopped
    env_file:
      - ./local-data/.env
    volumes:
      - ./nginx_101/nginx_certs:/etc/letsencrypt
      - ./nginx_101/nginx_html_proj/nginx:/var/www/certbot
    networks:
      - frontend
    depends_on:
      - nginx
    command: certonly --webroot --webroot-path=/var/www/certbot --email ${CERT_EMAIL} --agree-tos --no-eff-email -d ${DOMAIN_NAME} --rsa-key-size 4096
    # Uncomment to auto-renew (use with cron or systemd on host)
    # command: renew --webroot --webroot-path=/var/www/certbot --quiet